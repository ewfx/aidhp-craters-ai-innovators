# üöÄ Project Name
AI-Driven Hyper-Personalization & Recommendations

## üìå Table of Contents
- [Introduction](#introduction)
- [Demo](#demo)
- [Inspiration](#inspiration)
- [What It Does](#what-it-does)
- [How We Built It](#how-we-built-it)
- [Challenges We Faced](#challenges-we-faced)
- [How to Run](#how-to-run)
- [Tech Stack](#tech-stack)
- [Team](#team)

---

## üéØ Introduction
This project is a Flask-based intelligent recommendation system that leverages machine learning (ML), vector databases, and large language models (LLMs) to provide personalized financial recommendations. It predicts a customer's likelihood of applying for a home loan and generates tailored suggestions for home loans or investment products based on their financial profile and retrieved knowledge.

The purpose of this project is to address the lack of personalization in financial services by combining predictive analytics with context-aware recommendations. By integrating a Retrieval-Augmented Generation (RAG) pipeline, the system ensures that recommendations are both relevant and actionable.

## üé• Demo
üîó [Live Demo]: Live demo can be found in https://github.com/ewfx/aidhp-craters-ai-innovators/tree/main/artifacts/demo  
üìπ [Video Demo]: Demo recording can be found in : https://github.com/ewfx/aidhp-craters-ai-innovators/tree/main/artifacts/demo
üñºÔ∏è Screenshots:

![Screenshot 1]: For screenshots and ppt document refer the below link
https://github.com/ewfx/aidhp-craters-ai-innovators/tree/main/artifacts/demo  

## üí° Inspiration
   The inspiration for this project stems from the growing need for personalized financial services in the banking and financial sector. Customers today expect tailored recommendations that align with their unique financial profiles, goals, and circumstances. Traditional financial systems often rely on static rules or generic advice, which fails to address individual needs effectively.

   This project aims to bridge that gap by leveraging machine learning (ML), vector databases, and large language models (LLMs) to provide context-aware, personalized financial recommendations.

## ‚öôÔ∏è What It Does
Key Features and Functionalities of the Project
   This project is a Flask-based REST API that predicts a customer's likelihood of applying for a home loan and generates personalized financial recommendations using a Retrieval-Augmented Generation (RAG) pipeline. Below are the key features and functionalities:

1. **Home Loan Prediction**
   **Functionality:**
   Predicts whether a customer is "Likely to apply" or "Unlikely to apply" for a home loan based on their profile data.
   **How It Works:**
   A pre-trained machine learning model (home_loan_model.pkl) is used for prediction.
   Customer data is preprocessed:
   Missing values are filled with default values.
   Categorical features are encoded using pre-trained label encoders.
   Numerical features are scaled using a pre-trained scaler.

2. **Retrieval-Augmented Generation (RAG)**
   Functionality:
   Retrieves relevant context from a vector database (Chroma) to enhance the recommendations generated by the LLM.
   How It Works:
   Based on the prediction:
   If "Likely to apply," the system queries the vector store for Home Lending Products.
   If "Unlikely to apply," the system queries the vector store for Investment Products.
   The retrieved documents are used as context for the LLM to generate more informed recommendations.

3. **Personalized Recommendations**
   Functionality:
   Generates tailored financial recommendations using a Large Language Model (LLM).
   How It Works:
   The LLM (e.g., Llama) is invoked with a detailed prompt containing:
   Customer profile data.
   Retrieved context from the vector database.
   Recommendations are:
   Home Loan Options (if "Likely to apply").
   Investment Product Options (if "Unlikely to apply").
   The recommendations are concise, relevant, and tailored to the customer's profile.

4. **REST API Endpoint**
   Endpoint: /api/predict_home_loan
   Method: POST
   Input:
   Customer profile data in JSON format, including fields like age, income, account balance, employment status, etc.
   Output:
   A JSON response containing:
   Home Loan Likelihood: "Likely to apply" or "Unlikely to apply."
   Recommendations: A list of 3 personalized financial product suggestions.

5. **Vector Database Integration**
   Functionality:
   Stores and retrieves document embeddings for context-aware recommendations.
   How It Works:
   Uses Chroma as the vector database.
   Embeddings are generated using Hugging Face's sentence-transformers/all-MiniLM-L6-v2 model.
   The similarity_search method retrieves the most relevant documents based on the query.

6. **Machine Learning Integration**
   Functionality:
   Uses a pre-trained machine learning model to predict home loan likelihood.
   How It Works:
   The model is loaded using joblib.
   Preprocessing steps include:
   Encoding categorical features.
   Scaling numerical features.
   The prediction output is used to determine the type of recommendations to generate.

7. **Large Language Model (LLM) Integration**
   Functionality:
   Generates human-like recommendations based on customer data and retrieved context.
   How It Works:
   The LLM (e.g., Llama) is initialized using LangChain's init_chat_model.
   A detailed prompt is constructed with customer data and retrieved context.
   The LLM generates recommendations in a structured format.

8. **Data Preprocessing**
   Functionality:
   Ensures customer data is clean and ready for prediction.
   How It Works:
   Missing values are replaced with default values.
   Categorical features are encoded using pre-trained label encoders.
   Numerical features are scaled using a pre-trained scaler.

9. **Environment Management**
   Functionality:
   Manages sensitive information like API keys using environment variables.
   How It Works:
   Uses dotenv to load environment variables from a .env file (e.g., Hugging Face API key).

10. **Debugging and Logging**
   Functionality:
   Provides detailed logs for debugging and monitoring.
   How It Works:
   Logs key steps, such as:
   Adding documents to the vector store.
   Retrieving context from the vector store.
   Generating recommendations using the LLM.

**Example Workflow**
   Input:
   A customer submits their profile (e.g., age, income, account balance) via the /api/predict_home_loan endpoint.
   Prediction:
   The ML model predicts whether the customer is "Likely to apply" or "Unlikely to apply" for a home loan.
   Context Retrieval:
   Relevant documents (e.g., home loan or investment products) are retrieved from the vector database.
   Recommendation Generation:
   The LLM generates 3 personalized recommendations based on the profile and retrieved context.
   Output:
   The API returns the prediction and recommendations in JSON format.

**Key Benefits**
   Combines machine learning, vector databases, and LLMs for intelligent, context-aware recommendations.
   Provides a scalable and modular architecture for financial recommendation systems.
   Ensures personalized and relevant suggestions for customers based on their profiles and retrieved knowledge.

## üõ†Ô∏è How We Built It
Brief Summary of the Program
This program is a Flask-based REST API that predicts a customer's likelihood of applying for a home loan and generates personalized recommendations using a Retrieval-Augmented Generation (RAG) pipeline. It combines traditional machine learning, vector databases, and large language models (LLMs) to deliver context-aware recommendations.

**Key Functionalities**
**Home Loan Prediction:**

A pre-trained machine learning model (home_loan_model.pkl) predicts whether a customer is "Likely to apply" or "Unlikely to apply" for a home loan based on their profile data.
The profile data is preprocessed using a scaler and label encoders.
Retrieval-Augmented Generation (RAG):

The program uses a Chroma vector database to retrieve relevant context (e.g., "Home Lending Products" or "Investment Products") based on the prediction.
The retrieved context is passed to an LLM (e.g., Llama) to generate personalized recommendations.

**Personalized Recommendations:**

If the customer is "Likely to apply," the LLM suggests 3 suitable home loan options.
If "Unlikely to apply," the LLM suggests 3 suitable investment product options.
The recommendations are concise and tailored to the customer's profile and retrieved context.
REST API Endpoint:

Endpoint: /api/predict_home_loan
Method: POST
Input: Customer profile data in JSON format.
Output: A JSON response containing:
Home loan likelihood prediction.
Recommendations generated by the LLM.

**Technologies and Tools Used**

   Flask: For building the REST API.<br/>
   Joblib: For loading the pre-trained ML model, scaler, and label encoders.<br/>
   LangChain: For integrating the LLM and vector database.<br/>
   Chroma: A vector database for storing and retrieving document embeddings.<br/>
   Hugging Face: For generating embeddings using the sentence-transformers/all-MiniLM-L6-v2 model.<br/>
   LLM (Llama): For generating personalized recommendations based on the customer profile and retrieved context.<br/>
   NumPy and Pandas: For data preprocessing.<br/>
   dotenv: For managing environment variables (e.g., API keys).<br/>

**Workflow**
**Input**: Customer profile data is sent to the /api/predict_home_loan endpoint.
**Preprocessing**: The data is cleaned, encoded, and scaled.
**Prediction**: The ML model predicts the likelihood of applying for a home loan.
**Context Retrieval**: Relevant documents are retrieved from the Chroma vector database.
**LLM Recommendations**: The LLM generates recommendations based on the profile and retrieved context.
**Output**: The API returns the prediction and recommendations in JSON format.

**Example Use Case**
A customer submits their profile (e.g., age, income, account balance).
The API predicts they are "Likely to apply" for a home loan.
The system retrieves relevant home loan product details from the vector database.
The LLM suggests 3 suitable home loan options tailored to the customer's profile.

## üöß Challenges We Faced
Technical Challenges
Integration of Machine Learning and LLMs:

Challenge: Combining traditional machine learning (ML) models for prediction with large language models (LLMs) for generating recommendations required careful design.
Solution: Used a modular approach where the ML model predicts the likelihood, and the LLM generates recommendations based on the prediction and retrieved context.
Vector Database Integration (Chroma):

Challenge: Setting up and managing the Chroma vector database for storing and retrieving embeddings was complex, especially ensuring compatibility with Hugging Face embeddings.
Solution: Used LangChain's Chroma integration to simplify the process and ensure seamless retrieval of relevant documents.
Prompt Engineering for LLMs:

Challenge: Crafting effective prompts for the LLM to generate accurate and concise recommendations was iterative and time-consuming.
Solution: Designed structured prompts that included customer profiles, retrieved context, and clear instructions for the LLM.
Data Preprocessing:

Challenge: Handling missing or inconsistent customer data while ensuring compatibility with the pre-trained ML model.
Solution: Implemented default values for missing keys and used pre-trained label encoders and scalers to preprocess the data.
Performance Optimization:

Challenge: Ensuring the system responded quickly, especially when retrieving context from the vector database and invoking the LLM.
Solution: Limited the number of retrieved documents (k=1) and optimized the LLM invocation process to reduce latency.
Error Handling:

## üèÉ How to Run
1. Clone the repository  
   
   git clone https://github.com/ewfx/aidhp-craters-ai-innovators.git

   
2. Install dependencies  
   pip install flask joblib numpy pandas langchain langchain-community chromadb python-dotenv scikit-learn

   Make sure to create .venv (Virtual Environment) and install the dependencies in the virtual environment under src folder.<br/>

   Need API Key or Access Token form groqcloud and huggingface. Add GROQ_API_KEY and HF_TOKEN in .env file under src folder.

4. Run the project<br/>
   Open the src folder in VS Code and run the following command in the terminal:
   python3 rag_recomend_api.py

## üèóÔ∏è Tech Stack
Backend Framework: Flask <br/>
Machine Learning: Pre-trained model (joblib), Scaler, Label Encoders, Random forst algorithm<br/>
Vector Database: Chroma<br/>
Embeddings: Hugging Face Inference API<br/>
LLM: Llama (via LangChain)<br/>
Data Processing: NumPy, Pandas<br/>
Environment Management: dotenv<br/>
API Type: REST API<br/>

## üë• Team
- Karthik Neela 
- Bipul Biswas
- Simhachalam Lakkakula
- Brahmayya Nakka
- Sree Sushma Rupineni
